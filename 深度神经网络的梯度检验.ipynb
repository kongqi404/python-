{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "深度神经网络的梯度检验.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1WuZs7FGBlHmj8jSCLu_mj52qqUjZyFoY",
      "authorship_tag": "ABX9TyO21rUK2DWdDjhbIf3IUkyo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kongqiahaha/python-/blob/master/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b7QcOdGYwt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0,\"./drive/My Drive/data/深度神经网络_初始化_正则化_梯度检验\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPEpiIKW1EYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from testCases import *\n",
        "from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH8LpBnRZH2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_propagation(x, theta):\n",
        "  J=theta*x\n",
        "  return J\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekmNHFjKcvV7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f5dd7f50-7f51-422f-f307-6a92ffa4c0cc"
      },
      "source": [
        "x, theta = 2, 4\n",
        "J = forward_propagation(x, theta)\n",
        "print (\"J = \" + str(J))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "J = 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp1D0y9ygJ2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_propagation(x, theta):\n",
        "  dtheta=x\n",
        "  return dtheta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryeoP9YQgSeC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "16d66f8e-9e61-4cb8-8876-2f714a9b588b"
      },
      "source": [
        "x, theta = 2, 4\n",
        "dtheta = backward_propagation(x, theta)\n",
        "print (\"dtheta = \" + str(dtheta))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dtheta = 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyKfWEyZgm4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_check(x, theta, epsilon = 1e-7):\n",
        "  theta_plus=theta+epsilon\n",
        "  theta_minus=theta-epsilon\n",
        "  J_plus=forward_propagation(x,theta_plus)\n",
        "  J_minus=forward_propagation(x,theta_minus)\n",
        "  gradapprox=(J_plus-J_minus)/(2*epsilon)\n",
        "  grad=backward_propagation(x,theta)\n",
        "  numerator = np.linalg.norm(grad - gradapprox)                               # Step 1'\n",
        "  denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                            # Step 2'\n",
        "  difference = numerator / denominator  \n",
        "  if difference < 1e-7:\n",
        "    print (\"The gradient is correct!\")\n",
        "  else:\n",
        "    print (\"The gradient is wrong!\") \n",
        "  return difference"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Dw3weAIjldk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "bfc0d05e-0c66-46da-b82d-046b5cdce86a"
      },
      "source": [
        "x, theta = 2, 4\n",
        "difference = gradient_check(x, theta)\n",
        "print(\"difference = \" + str(difference))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The gradient is correct!\n",
            "difference = 2.919335883291695e-10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBQvYElwjteL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_propagation_n(X, Y, parameters):\n",
        "  m=X.shape[1]\n",
        "  W1 = parameters[\"W1\"]\n",
        "  b1 = parameters[\"b1\"]\n",
        "  W2 = parameters[\"W2\"]\n",
        "  b2 = parameters[\"b2\"]\n",
        "  W3 = parameters[\"W3\"]\n",
        "  b3 = parameters[\"b3\"]\n",
        "  Z1 = np.dot(W1, X) + b1\n",
        "  A1 = relu(Z1)\n",
        "  Z2 = np.dot(W2, A1) + b2\n",
        "  A2 = relu(Z2)\n",
        "  Z3 = np.dot(W3, A2) + b3\n",
        "  A3 = sigmoid(Z3)\n",
        "  logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
        "  cost = 1./m * np.sum(logprobs)\n",
        "  cache=(Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
        "  return cost,cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1rr6pt8mcX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_propagation_n(X, Y, cache):\n",
        "  m = X.shape[1]\n",
        "  (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)=cache\n",
        "  dZ3=A3-Y\n",
        "  dW3 = 1./m * np.dot(dZ3, A2.T)\n",
        "  db3=(1./m)*np.sum(dZ3,axis=1,keepdims=True)\n",
        "  dA2=np.dot(W3.T,dZ3)\n",
        "  dZ2=np.multiply(dA2,np.int64(A2>0))\n",
        "  dW2=1./m*np.dot(dZ2,A1.T)*2\n",
        "  db2=1./m*np.sum(dZ2,axis=1,keepdims=True)\n",
        "  dA1=np.dot(W2.T,dZ2)\n",
        "  dZ1=np.multiply(dA1,np.int64(A1>0))\n",
        "  dW1=1./m*np.dot(dZ1,X.T)\n",
        "  db1=4./m*np.sum(dZ1,axis=1,keepdims=True)\n",
        "  gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
        "          \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
        "          \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
        "  return gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opFPgnmrouuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):\n",
        "  parameters_values, _ = dictionary_to_vector(parameters)\n",
        "  grad = gradients_to_vector(gradients)\n",
        "  num_parameters = parameters_values.shape[0]\n",
        "  J_plus = np.zeros((num_parameters, 1))\n",
        "  J_minus=np.zeros((num_parameters, 1))\n",
        "  gradapprox = np.zeros((num_parameters, 1))\n",
        "  for i in range(num_parameters):\n",
        "    thetaplus = np.copy(parameters_values)\n",
        "    thetaplus[i][0] += epsilon\n",
        "    J_plus[i],_=forward_propagation_n(X,Y,vector_to_dictionary(thetaplus))\n",
        "\n",
        "    thetaminus=np.copy(parameters_values)\n",
        "    thetaminus[i][0] -=epsilon\n",
        "    J_minus[i],_=forward_propagation_n(X,Y,vector_to_dictionary(thetaminus))\n",
        "\n",
        "    gradapprox[i]= (J_plus[i]-J_minus[i])/(2.*epsilon)\n",
        "  numerator = np.linalg.norm(grad - gradapprox)                                           # Step 1'\n",
        "  denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                                         # Step 2'\n",
        "  difference = numerator / denominator\n",
        "\n",
        "  if difference > 1e-7:\n",
        "    print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
        "  else:\n",
        "    print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
        "\n",
        "  return difference"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cztqoFUJwM7l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "43f989da-fbf1-44b0-cfda-1a545bea7753"
      },
      "source": [
        "X, Y, parameters = gradient_check_n_test_case()\n",
        "\n",
        "cost, cache = forward_propagation_n(X, Y, parameters)\n",
        "gradients = backward_propagation_n(X, Y, cache)\n",
        "difference = gradient_check_n(parameters, gradients, X, Y)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[93mThere is a mistake in the backward propagation! difference = 0.2850931567761624\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}